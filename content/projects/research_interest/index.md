---
title: Pre-Trained Language Models and News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

## Introduction

In recent years, large-scale pre-trained language models such as BERT [1] and GPT [2] have been rapidly developed and implemented to our society.
The use of artificial intelligence technology, including large-scale pre-trained language models, has always been discussed in the world of journalism [3].
My research interest is to explore the opportunities and challenges of pre-trained language models in the news media.

## Featured Work

### Pre-Training Language Models

To gain a deeper understanding of pre-trained language models, it is important not only to fine-tune publicly available models, but also to address pre-training.
We build language models using a corpus of Japanese news articles [4] and evaluate the potential for the help of journalism.
In particular, the need for model reconstruction due to the semantic changes is investigated [5].

### Fine-tuning Language Models

Pre-trained language models are fine-tuned for individual tasks:

- Summarization [6]
- Sentence or Document Scoring [7][8]

## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for the detail.

- [1] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North AMerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86.
- [2] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al (2020). Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.
- [3] New Powers, New Responsibilities. A Global Survey of Journalism and Artificial Intelligence. (2019). Polis. November 18, 2019. https://blogs.lse.ac.uk/polis/2019/11/18/new-powers-new-responsibilities/.
- [4] 石原慧人, 石原祥太郎, 白井穂乃 (2021). BertSumを用いた日本語ニュース記事の抽象型要約手法の検討. 2021年度人工知能学会全国大会（第35回）論文集.
- [5] 高橋寛武, 石原祥太郎, 白井穂乃 (2022). 単語分散表現を用いた新型コロナウイルスによる意味変化検出. 言語処理学会第28回年次大会発表論文集.
- [6] Shotaro Ishihara, Yuta Matsuda, and Norihiko Sawa (2021). Editors-in-the-loop News Article Summarization Framework with Sentence Selection and Compression. In Proceedings of the 5th IEEE Workshop on Human-in-the-Loop Methods and Future of Work in BigData, 3522-24.
- [7] Shotaro Ishihara and Yasufumi Nakama (2022). Generating a Pairwise Dataset for Click-through Rate Prediction of News Articles Considering Positions and Contents. In Proceedings of Computation + Journalism Conference 2022. ACM, New York, NY, USA, 5 pages. (to appear)
- [8] Shotaro Ishihara, and Hono Shirai (2022). Nikkei at SemEval-2022 Task 8: Exploring BERT-based Bi-Encoder Approach for Pairwise Multilingual News Article Similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022). (to appear)
