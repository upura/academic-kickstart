---
title: AI-Accelerated News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

My research interests lie in the realization of AI-accelerated news media.
This involves creating new experiences and improving operational efficiency in all stages of news production.
In particular, we focus on exploring opportunities and challenges for pre-trained models in news media.

### Building and Applying Information Technology Such as Pre-trained Models

<iframe class="speakerdeck-iframe" frameborder="0" src="https://speakerdeck.com/player/7c1fff569f0449c6bd056f0ec45f57e1" title="ニュースメディアにおける事前学習済みモデルの可能性と課題 / IBIS2024" allowfullscreen="true" style="border: 0px; background: padding-box padding-box rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;" data-ratio="1.7777777777777777"></iframe>

- Domain-specific pre-trained models ([Press release](https://www.nikkei.co.jp/nikkeiinfo/en/news/press/release_en_20240424_01.pdf))
- News summarization ([Journal of Natural Language Processing](https://doi.org/10.5715/jnlp.31.1717))
- Entity linking system ([Journal of Natural Language Processing](https://doi.org/10.5715/jnlp.31.1330))
- Crossword puzzle generation ([CIKM 2023](https://dl.acm.org/doi/10.1145/3583780.3615151))
- Semantic shift analysis ([IC2S2 2023](https://upura.github.io/pdf/ic2s2_2023_semantic_shift.pdf))
- Reading time estimation ([BigData 2022 Industrial & Government Track](https://ieeexplore.ieee.org/document/10020618))

### Challenges in Practice

<iframe class="speakerdeck-iframe" frameborder="0" src="https://speakerdeck.com/player/3813f4b9e4af4f328a96f2fdb9484bd8" title="日本語新聞記事を用いた大規模言語モデルの暗記定量化 / LLMC2025" allowfullscreen="true" style="border: 0px; background: padding-box padding-box rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;" data-ratio="1.7777777777777777"></iframe>

- Quantifying memorization of LLMs
    - Survey paper ([ACL 2023 Workshop](https://aclanthology.org/2023.trustnlp-1.23/))
    - Experiments on Japanese newspaper ([INLG 2024](https://aclanthology.org/2024.inlg-main.14/) & [ACL 2025 Workshop](https://aclanthology.org/2025.l2m2-1.8/))
    - Monitoring time-series performance degradation ([AACL-IJCNLP 2022](https://aclanthology.org/2022.aacl-main.17/) & [Journal of Natural Language Processing](https://doi.org/10.5715/jnlp.31.1563))
    - Experiments on Japanese newspaper ([INLG 2024](https://aclanthology.org/2024.inlg-main.14/), [ACL 2025 Workshop](https://aclanthology.org/2025.l2m2-1.8/))
- Hallucination analysis ([Journal of Natural Language Processing](https://doi.org/10.5715/jnlp.31.1717))
- Revisiting news recommendation ([NLDB 2025](https://doi.org/10.1007/978-3-031-97144-0_16))

## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for more details.
