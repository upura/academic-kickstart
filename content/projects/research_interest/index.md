---
title: Pre-Trained Language Models and News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

My research interest is to explore the opportunities and challenges of pre-trained language models in the news media.

### Challenges in Practice

- Monitoring time-series performance degradation ([AACL-IJCNLP 2022](https://aclanthology.org/2022.aacl-main.17/))
- Training Data Extraction ([ACL 2023 Workshop](https://aclanthology.org/2023.trustnlp-1.23/))
- Adversarial Validation ([SIGIR 2021 Workshop](https://sigir-ecom.github.io/ecom2021/accepted-papers.html))

### Applied Machine Learning

- Editors-in-the-loop Summarization ([BigData 2021 Workshop](https://ieeexplore.ieee.org/document/9671300))
- Reading Time Estimation ([BigData 2022 Industrial & Government Track](https://ieeexplore.ieee.org/document/10020618))
- Multilingual News Similarity Scoring ([NAACL 2022 Workshop](https://aclanthology.org/2022.semeval-1.171/))
- Next Item Recommendation ([WSDM 2021 Workshop](https://ceur-ws.org/Vol-2855/challenge_short_7.pdf))
- Quantifying Diachronic Language Change ([Ishihara et al., IC2S2 2023](https://www.ic2s2.org/))

## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for the detail.
