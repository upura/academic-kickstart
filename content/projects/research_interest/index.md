---
title: Pre-Trained Language Models and News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

My research interest is to explore the opportunities and challenges of pre-trained language models (PLMs) in the news media.

<iframe class="speakerdeck-iframe" frameborder="0" src="https://speakerdeck.com/player/7c1fff569f0449c6bd056f0ec45f57e1" title="ニュースメディアにおける事前学習済みモデルの可能性と課題 / IBIS2024" allowfullscreen="true" style="border: 0px; background: padding-box padding-box rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;" data-ratio="1.7777777777777777"></iframe>

### Challenges in Practice

- Monitoring time-series performance degradation ([AACL-IJCNLP 2022](https://aclanthology.org/2022.aacl-main.17/) & [IC2S2 2023](https://upura.github.io/pdf/ic2s2_2023_semantic_shift.pdf) => Journal of Natural Language Processing)
- Training data extraction
    - Survey paper ([ACL 2023 Workshop](https://aclanthology.org/2023.trustnlp-1.23/))
    - Experiments on Japanese newspaper ([INLG 2024](https://arxiv.org/abs/2404.17143v2))
- Hallucination analysis on domain-specific PLMs (Journal of Natural Language Processing)

### Building and Applying PLMs

- Building domain-specific PLMs ([Press release](https://www.nikkei.co.jp/nikkeiinfo/en/news/press/release_en_20240424_01.pdf) & Journal of Natural Language Processing)
- PLMs for news industry:
    - Crossword puzzle generation ([CIKM 2023](https://dl.acm.org/doi/10.1145/3583780.3615151))
    - Reading time estimation ([BigData 2022 Industrial & Government Track](https://ieeexplore.ieee.org/document/10020618))
    - Multilingual news similarity scoring ([NAACL 2022 Workshop](https://aclanthology.org/2022.semeval-1.171/))
    - Next Item Recommendation ([WSDM 2021 Workshop](https://ceur-ws.org/Vol-2855/challenge_short_7.pdf))
S
## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for the detail.
