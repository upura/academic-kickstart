---
title: Pre-Trained Language Models and News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

My research interest is to explore the opportunities and challenges of pre-trained language models in the news media.

### Challenges in Practice

- Monitoring time-series performance degradation ([Ishihara et al., AACL-IJCNLP 2022](https://aclanthology.org/2022.aacl-main.17/))
- Training Data Extraction (Ishihara, ANLP 2023)
- Adversarial Validation ([Ishihara et al., SIGIR 2021 Workshop](https://sigir-ecom.github.io/ecom2021/accepted-papers.html))

### Applied Machine Learning

- Editors-in-the-loop Summarization ([Ishihara et al., BigData 2021 Workshop](https://ieeexplore.ieee.org/document/9671300))
- Reading Time Estimation ([Ishihara et al., BigData 2022 Industrial & Government Track](https://ieeexplore.ieee.org/document/10020618))
- Multilingual News Similarity Scoring ([Ishihara et al., NAACL 2022 Workshop](https://aclanthology.org/2022.semeval-1.171/))
- Next Item Recommendation ([Ishihara et al., WSDM 2021 Workshop](https://ceur-ws.org/Vol-2855/challenge_short_7.pdf))
- Click-through Rate Prediction ([Ishihara et al., CJ 2022](https://cj2022.brown.columbia.edu/proceedings/))
- User Attribute Prediction ([Ishihara et al., CJ 2020](https://cj2021.northeastern.edu/research-papers/))
- Quantifying Diachronic Language Change ([Ishihara et al., IC2S2 2023](https://www.ic2s2.org/))

## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for the detail.
