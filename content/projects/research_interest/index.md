---
title: Pre-Trained Language Models and News Media
summary:
tags:
date: "2021-05-01T00:00:00Z"

---

My research interest is to explore the opportunities and challenges of pre-trained language models (PLMs) in the news media.

### Challenges in Practice

- Monitoring time-series performance degradation ([AACL-IJCNLP 2022](https://aclanthology.org/2022.aacl-main.17/) & [IC2S2 2023](https://upura.github.io/pdf/ic2s2_2023_semantic_shift.pdf) => Journal of Natural Language Processing)
- Training data extraction
    - Survey paper ([ACL 2023 Workshop](https://aclanthology.org/2023.trustnlp-1.23/))
    - Experiments on Japanese newspaper ([INLG 2024](https://arxiv.org/abs/2404.17143v2))

### Building and Applying PLMs

- Building domain-specific PLMs ([Press release](https://www.nikkei.co.jp/nikkeiinfo/en/news/press/release_en_20240424_01.pdf) & Journal of Natural Language Processing)
- PLMs for news industry:
    - Crossword puzzle generation ([CIKM 2023](https://dl.acm.org/doi/10.1145/3583780.3615151))
    - Reading time estimation ([BigData 2022 Industrial & Government Track](https://ieeexplore.ieee.org/document/10020618))
    - Multilingual news similarity scoring ([NAACL 2022 Workshop](https://aclanthology.org/2022.semeval-1.171/))
    - Next Item Recommendation ([WSDM 2021 Workshop](https://ceur-ws.org/Vol-2855/challenge_short_7.pdf))

## Reference

You can see [Publications](https://upura.github.io/projects/publications/) for the detail.
